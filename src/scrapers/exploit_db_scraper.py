from bs4 import BeautifulSoup
import aiohttp
from typing import List, Dict, Any
from .base_scraper import BaseScraper
import json
import re
from urllib.parse import quote
import asyncio
import time

class ExploitDBScraper(BaseScraper):
    def __init__(self):
        super().__init__()
        self.base_url = "https://www.exploit-db.com"
        self.api_url = f"{self.base_url}/search"
        self.min_request_interval = 5  # Increased delay for ExploitDB
        self.last_request_time = 0
        
    async def _wait_for_rate_limit(self):
        """Ensure we wait appropriate time between requests"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        if time_since_last_request < self.min_request_interval:
            await asyncio.sleep(self.min_request_interval - time_since_last_request)
        self.last_request_time = time.time()
        
    async def search(self, query: str) -> List[Dict[str, Any]]:
        results = []
        
        try:
            await self._wait_for_rate_limit()
            # Try web scraping first as it's more reliable
            web_results = await self._search_web(query)
            if web_results:
                results.extend(web_results)
                
            # If web scraping fails, try API
            if not results:
                api_results = await self._search_api(query)
                results.extend(api_results)
                
        except Exception as e:
            print(f"Error searching ExploitDB: {str(e)}")
            
        return results
        
    async def _search_api(self, query: str) -> List[Dict[str, Any]]:
        """Search using ExploitDB API"""
        results = []
        version = self._extract_version(query)
        
        headers = {
            **self.headers,
            'Accept': 'application/json',
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': self.base_url
        }
        
        params = {
            'term': query,
            'platform': '',
            'type': '',
            'tag': '',
            'author': '',
            'verified': '',
            'port': '',
            'draw': 1,
            'length': 10,
            'start': 0
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                await self._wait_for_rate_limit()
                async with session.get(self.api_url, params=params, headers=headers, ssl=False) as response:
                    if response.status == 200:
                        data = await response.json()
                        exploits = data.get('data', [])
                        
                        for exploit in exploits:
                            # If version is specified, check if it's mentioned
                            if version and version not in str(exploit):
                                continue
                                
                            exploit_id = exploit.get('id', {}).get('text', 'N/A')
                            description = exploit.get('description', {}).get('text', 'N/A')
                            title = exploit.get('title', {}).get('text', 'N/A')
                            
                            results.append(self._parse_exploit(exploit_id, title, description))
                            
        except Exception as e:
            print(f"Error in ExploitDB API search: {str(e)}")
            
        return results
        
    async def _search_web(self, query: str) -> List[Dict[str, Any]]:
        """Search using web interface"""
        results = []
        encoded_query = quote(query)
        search_url = f"{self.base_url}/search?q={encoded_query}"
        
        headers = {
            **self.headers,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
            'Referer': self.base_url
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                await self._wait_for_rate_limit()
                async with session.get(search_url, headers=headers, ssl=False) as response:
                    if response.status == 200:
                        content = await response.text()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # Find exploit table
                        exploit_table = soup.find('table', {'id': 'exploits-table'})
                        if not exploit_table:
                            # Try alternate table class
                            exploit_table = soup.find('table', {'class': 'exploit_list'})
                            
                        if exploit_table:
                            for row in exploit_table.find_all('tr')[1:]:  # Skip header row
                                try:
                                    cols = row.find_all('td')
                                    if len(cols) >= 5:
                                        exploit_link = cols[1].find('a')
                                        if not exploit_link:
                                            continue
                                            
                                        exploit_id = exploit_link.get('href', '').split('/')[-1]
                                        title = exploit_link.text.strip()
                                        description = cols[2].text.strip() if len(cols) > 2 else title
                                        
                                        # Get additional details from the exploit page
                                        await self._wait_for_rate_limit()
                                        details_url = f"{self.base_url}/exploits/{exploit_id}"
                                        
                                        try:
                                            async with session.get(details_url, headers=headers, ssl=False) as details_response:
                                                if details_response.status == 200:
                                                    details_content = await details_response.text()
                                                    details_soup = BeautifulSoup(details_content, 'html.parser')
                                                    
                                                    # Try to get a better description from the details page
                                                    detail_desc = details_soup.find('div', {'class': 'card-body'})
                                                    if detail_desc:
                                                        description = detail_desc.text.strip()
                                                        
                                        except Exception as e:
                                            print(f"Error fetching exploit details for {exploit_id}: {str(e)}")
                                            
                                        results.append(self._parse_exploit(exploit_id, title, description))
                                        
                                except Exception as e:
                                    print(f"Error parsing exploit row: {str(e)}")
                                    continue
                                    
        except Exception as e:
            print(f"Error in ExploitDB web search: {str(e)}")
            
        return results
        
    def _parse_exploit(self, exploit_id: str, title: str, description: str) -> Dict[str, Any]:
        """Parse exploit data into common format"""
        exploit_url = f"{self.base_url}/exploits/{exploit_id}"
        return {
            'title': f"ExploitDB: {title}",
            'description': description,
            'source': 'ExploitDB',
            'url': exploit_url,
            'exploit_id': exploit_id,
            'references': [
                {'url': exploit_url, 'source': 'ExploitDB Entry'},
                {'url': f"{self.base_url}/raw/{exploit_id}", 'source': 'Raw Exploit'},
                {'url': f"{self.base_url}/download/{exploit_id}", 'source': 'Download Exploit'}
            ]
        }
        
    def _extract_version(self, query: str) -> str:
        """Extract version number from query"""
        version_match = re.search(r'(\d+\.(?:\d+\.)*\d+)', query)
        return version_match.group(1) if version_match else None 